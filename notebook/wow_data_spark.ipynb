{
  "metadata": {
    "saveOutput": true,
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [],
      "metadata": {},
      "source": [
        "wow_log_df = spark.read.load('abfss://dapfilesystem@cloocus.dfs.core.windows.net/wowjs/WoW_Logs.csv', format='csv'\n",
        "## If header exists uncomment line below\n",
        ", header=True\n",
        ")\n",
        "churnersdf = spark.read.load('abfss://dapfilesystem@cloocus.dfs.core.windows.net/wowjs/churners.csv', format='csv'\n",
        "## If header exists uncomment line below\n",
        ", header=True\n",
        ")\n",
        "zones_df = spark.read.load('abfss://dapfilesystem@cloocus.dfs.core.windows.net/wowjs/zones.csv', format='csv'\n",
        "## If header exists uncomment line below\n",
        ", header=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 전처리 과정\n",
        "컬럼이름 변경\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [],
      "metadata": {},
      "source": [
        "wow_log_df = wow_log_df.withColumnRenamed('char', 'Identifier')\n",
        "wow_log_df = wow_log_df.withColumnRenamed('zone', 'zoneId')\n",
        "wow_log_df = wow_log_df.withColumnRenamed('timestamp', 'log_timestamp')\n",
        "churnersdf = churnersdf.withColumnRenamed('char', 'Identifier')\n",
        "churnersdf = churnersdf.withColumnRenamed('timestamp', 'churn_timestamp')\n",
        "zones_df = zones_df.withColumnRenamed('Zone_Name', 'zoneId')\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 전처리 과정\n",
        "Exctract value in \"()\""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [],
      "metadata": {},
      "source": [
        "%%pyspark\n",
        "from pyspark.sql import functions\n",
        "\n",
        "#wow_log_df.withColumn('char', functions.regexp_replace(wow_log_df.col(\"char\"),\"(\",\"\")).show(5)[]\n",
        "wow_log_df = wow_log_df.withColumn('Identifier', functions.regexp_replace(\"Identifier\",\"IdentifierId\\(\",\"\"))\n",
        "wow_log_df = wow_log_df.withColumn('Identifier', functions.regexp_replace(\"Identifier\",\"\\)\",\"\"))\n",
        "wow_log_df = wow_log_df.withColumn('zoneId', functions.regexp_replace(\"zoneId\",\"ZoneidId\\(\",\"\"))\n",
        "wow_log_df = wow_log_df.withColumn('zoneId', functions.regexp_replace(\"zoneId\",\"\\)\",\"\"))\n",
        "\n",
        "churnersdf = churnersdf.withColumn('Identifier', functions.regexp_replace(\"Identifier\",\"IdentifierId\\(\",\"\"))\n",
        "churnersdf = churnersdf.withColumn('Identifier', functions.regexp_replace(\"Identifier\",\"\\)\",\"\"))\n",
        "zones_df = zones_df.withColumn('zoneId', functions.regexp_replace(\"zoneId\",\"ZoneidId\\(\",\"\"))\n",
        "zones_df = zones_df.withColumn('zoneId', functions.regexp_replace(\"zoneId\",\"\\)\",\"\"))\n",
        "#wow_log_df.select(functions.regexp_replace('char', \"\\(\", \" \")).show()\n",
        "#wow_log_df.select(functions.regexp_replace('char', \"\\)\", \"\")).show()\n",
        "#wow_log_df.show(10)\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 전처리 과정\n",
        "wow_log data와 churner data Identifier join\n",
        "\n",
        "wow_log data와 zones_df data zoneId join\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [],
      "metadata": {},
      "source": [
        "wow_log_join = wow_log_df.join(churnersdf, wow_log_df.Identifier == churnersdf.Identifier).drop(churnersdf.Identifier)\n",
        "wow_log_result_join = wow_log_join.join(zones_df, wow_log_join.zoneId == zones_df.zoneId).drop(zones_df.zoneId)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [],
      "metadata": {},
      "source": [
        "wow_log_result_pd = wow_log_result_join.toPandas().to_csv('training_pd.csv', index=False)\n",
        "wow_log_result_pd.write.saveAsTable('wowdatatest')"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [],
      "metadata": {},
      "source": [
        "from azureml.core import Workspace\n",
        "subscription_id = \"6db530e8-7969-4378-9f96-13c9439e24e7\" #you should be owner or contributor\n",
        "resource_group = \"RG-Synapse-DAP-T\" #you should be owner or contributor\n",
        "workspace_name = \"clooml\" #your workspace name\n",
        "workspace_region = \"korea central\" #your region\n",
        "\n",
        "ws = Workspace(workspace_name = workspace_name,\n",
        "               subscription_id = subscription_id,\n",
        "               resource_group = resource_group)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "Uploading an estimated of 1 files\nUploading training_pd.csv\nUploaded training_pd.csv, 1 files out of an estimated total of 1\nUploaded 1 files\n$AZUREML_DATAREFERENCE_c5ddfdeb8eea472da61d56473a2964ee"
          },
          "execution_count": 10,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "datastore = ws.get_default_datastore()\n",
        "datastore.upload_files(files = ['training_pd.csv'],\n",
        "                       target_path = 'train-dataset/tabular/',\n",
        "                       overwrite = True,\n",
        "                       show_progress = True)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "{\n  \"name\": \"workspaceblobstore\",\n  \"container_name\": \"azureml-blobstore-3c6eb850-6607-4abf-abbd-cf3a6fea3185\",\n  \"account_name\": \"clooml1844864370\",\n  \"protocol\": \"https\",\n  \"endpoint\": \"core.windows.net\"\n}"
          },
          "execution_count": 9,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "df.write.format(\"com.databricks.spark.sqldw\").option(\"useAzureMSI\", \"true\").mode(\"append\").option(\"url\", url).option(\"dbtable\", dbtable).option(\"tempDir\", \"abfss://tempcontainer@adls77.dfs.core.windows.net/temp\").save()"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IllegalArgumentException",
          "evalue": "Expected exactly one path to be specified, but got: ",
          "traceback": [
            "IllegalArgumentException : Expected exactly one path to be specified, but got: ",
            "Traceback (most recent call last):\n",
            "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py\", line 737, in save\n    self._jwrite.save()\n",
            "  File \"/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n",
            "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 85, in deco\n    raise IllegalArgumentException(s.split(': ', 1)[1], stackTrace)\n",
            "pyspark.sql.utils.IllegalArgumentException: Expected exactly one path to be specified, but got: \n"
          ]
        }
      ],
      "metadata": {},
      "source": [
        "#https://docs.microsoft.com/ko-kr/azure/synapse-analytics/spark/apache-spark-azure-machine-learning-tutorial\n",
        "inner_join.write.format('csv').mode(\"append\").option(\"dapfilesystem\", \"abfss://dapfilesystem@cloocus.dfs.core.windows.net/wowjs\").save()"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "outputs": [],
      "metadata": {},
      "source": [
        "wow_log_df = spark.read.load('abfss://dapfilesystem@cloocus.dfs.core.windows.net/wowjs/abcde.csv', format='csv'\n",
        "## If header exists uncomment line below\n",
        ", header=False\n",
        ")"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "DataFrame[_c0: string, _c1: string, _c2: string, _c3: string, _c4: string, _c5: string, _c6: string, _c7: string, _c8: string, _c9: string, _c10: string, _c11: string, _c12: string]"
          },
          "execution_count": 18,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "wow_log_df"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        ""
      ],
      "attachments": {}
    }
  ]
}